{"cells":[{"cell_type":"markdown","metadata":{"id":"yCyuDv6ElAZf"},"source":["# Introduction to Textmining with NLTK\n","\n","A short introduction in data processing for textual data and some basic applications for sentiment analysis and"]},{"cell_type":"markdown","metadata":{"id":"BkZvhqMr_7jp"},"source":["# Basic Setup\n","\n","\n","Install nltk library for text processing and download some extensions that are required. Also, we install the wordcloud library for plotting our results as wordcloud."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhsMSw0lAZo"},"outputs":[],"source":["!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","!pip install textblob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjYyB1uE_7js"},"outputs":[],"source":["# we import a series of specific functions from the nltk package for processing the texts.\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import SnowballStemmer\n","from nltk import FreqDist\n","\n","# we import pandas for reading in files\n","import pandas as pd\n","\n","#textblob for performing more advanced Sentiment Analysis.\n","from textblob import TextBlob"]},{"cell_type":"markdown","metadata":{"id":"c8XRycrYlAZs"},"source":["## Read in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xIqDR9DlAZs"},"outputs":[],"source":["corpus = pd.read_csv(\"https://github.com/casbdai/notebooks2023/raw/main/Module2/Textmining/fake_news.csv\")\n","corpus.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"clREJQkKlAZs"},"source":["We extract the second document and save it as an object text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYc54tyNlAZt"},"outputs":[],"source":["______\n","print(______)"]},{"cell_type":"markdown","metadata":{"id":"7JWYdbmQlAZt"},"source":["## Pre-Processing Textual Data"]},{"cell_type":"markdown","metadata":{"id":"ssGwOsb6_7jv"},"source":["### Convert text to lower case:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-0nNcOW_7jw"},"outputs":[],"source":["lower_text = ______.______\n","print (lower_text)"]},{"cell_type":"markdown","metadata":{"id":"JkSpTRuC_7j0"},"source":["### Tokenize text\n","\n","Break down text into tokens, i.e, breaking the sentences into single words for analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dX9pJ-0x_7j1"},"outputs":[],"source":["word_tokens = ______.______\n","print (word_tokens)"]},{"cell_type":"markdown","metadata":{"id":"fSE97tGG_7j3"},"source":["We need a better tokenizer also \"punctuation\" and \"numbers\" are retained as tokens. Also, very short words are translated into tokens.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_SjwHcM_7j4"},"outputs":[],"source":["better_tokenizer = RegexpTokenizer(r'[a-zA-Z]{3,}')\n","\n","# [a-zA-Z] means that only letters are retained as tokens\n","# {3,} means that only tokens with at least three characters are retained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWDHnw5wlAZv"},"outputs":[],"source":["word_tokens = ______.______\n","print(word_tokens)"]},{"cell_type":"markdown","metadata":{"id":"EUvyYiX6_7j6"},"source":["## Remove stop words\n","\n","Remove irrelevant words using nltk stop words like is,the,a etc from the sentences as they donâ€™t carry any information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nIymk1QOlAZw"},"outputs":[],"source":["stopword = ______.______\n","stopword"]},{"cell_type":"markdown","metadata":{"id":"yZOlPzevlAZw"},"source":["For getting rid of stopwords, we must compare each token against the words in the stop words list. With can be easily done in a list comprehension. List comprehension are a common extension of \"for-loops\".\n","\n","A for loop that prints out every token:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCjMf97BlAZw"},"outputs":[],"source":["_________"]},{"cell_type":"markdown","metadata":{"id":"ixyraaB6lAZx"},"source":["Reformulating the for loop as a list comprehension. List comprehensions are considered to be very understandable and are thus used very frequently by pythonistas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EoK0NfSlAZx"},"outputs":[],"source":["_________"]},{"cell_type":"markdown","metadata":{"id":"UCP8eHeZlAZx"},"source":["Extending our list comprehension such that only tokens are retained that are NOT on the stoplist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_kUc5Ge_7j7"},"outputs":[],"source":["clean_tokens = [word for word in word_tokens ______]\n","print (clean_tokens)"]},{"cell_type":"markdown","metadata":{"id":"YWv-4wbB_7j9"},"source":["## Stemming\n","\n","\n","Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNYExNQ0_7kA"},"outputs":[],"source":[" ______"]},{"cell_type":"markdown","metadata":{"id":"PM5dPmVK_7kC"},"source":["## Get word frequency\n","\n","Counting the most frequently used words in a textdocument"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46OcQjjl_7kE"},"outputs":[],"source":["freq = FreqDist(stemmed_tokens)\n","print (freq.most_common(5))"]},{"cell_type":"markdown","metadata":{"id":"M6VoToo9lAZ1"},"source":["# Very Basic Sentiment Analysis\n","\n","Using a dictionairy of positive and negative words, we can now perform a very basic sentiment analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2LlUTIVlAZ1"},"outputs":[],"source":["______"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4S4SZpZylAZ1"},"outputs":[],"source":["neg_sent = []\n","\n","[neg_sent.append(1) for word in stemmed_tokens if word in [\"virus\", \"infect\",\"gun\"] ]\n","\n","sum(neg_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1lYkZrPlAZ1"},"outputs":[],"source":["______"]},{"cell_type":"markdown","metadata":{"id":"VuPyUg-tQkID"},"source":["## Better Sentiment Analysis with TextBlob\n","\n","Let's have a look at the original text we started with:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPrJ5zp0Qp8Y"},"outputs":[],"source":["text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKsdPDFyRCV8"},"outputs":[],"source":["______.______"]},{"cell_type":"markdown","metadata":{"id":"PDVvB4VZRsZS"},"source":["TextBlob returns polarity and subjectivity of a sentence.\n","\n","- Polarity lies between -1 and 1; -1 defines a negative sentiment and 1 defines a positive sentiment.\n","- Subjectivity lies between 0 and 1. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information."]},{"cell_type":"markdown","metadata":{"id":"b0yED-ukRuGi"},"source":["Apply Textblob Sentiment Analyis to the 4th File in the data set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDAClQPlR3km"},"outputs":[],"source":["text = corpus[\"text\"][3]\n","TextBlob(text).sentiment"]},{"cell_type":"markdown","metadata":{"id":"C3EDw4m9SFMu"},"source":["## Apply Textblob to entire column in pandas dataframe"]},{"cell_type":"markdown","metadata":{"id":"pDh8fy3kSNbm"},"source":["Pandas DataFrames cannot directly ingested into Textblob because it expects a different format of the data. But we can create two small helper functions that allow us to adapt textblob to a Pandas DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcDKQTn5SQtv"},"outputs":[],"source":["# function that only returns the polarity score of TextBlob\n","def polarity(text):\n","    try:\n","        return TextBlob(text).sentiment.polarity\n","    except:\n","        return None\n","\n","# function that only returns the subjectivity score of TextBlob\n","def subjectivity(text):\n","    try:\n","        return TextBlob(text).sentiment.subjectivity\n","    except:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2BOzopmSVwC"},"outputs":[],"source":["______"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8UP9KpuSZUE"},"outputs":[],"source":["corpus[______]= corpus[\"text\"].______(______)\n","corpus.head()"]},{"cell_type":"markdown","metadata":{"id":"poYDXngJSnx6"},"source":["## Comparing Fake and Real News"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rdnaq-t2SooE"},"outputs":[],"source":["______"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-O7rix7Soro"},"outputs":[],"source":["corpus.groupby(\"label\")[[\"polarity\", \"subjectivity\"]].mean().______"]}],"metadata":{"colab":{"provenance":[{"file_id":"1l3avI8VCGX3KQ5gyt2UMkafeGCa0UOkw","timestamp":1578350291282}]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"vscode":{"interpreter":{"hash":"9cfc3c7994f631dfc6a65b56363e87144dd9fa5c38ebff28a3247fb8dab8888e"}}},"nbformat":4,"nbformat_minor":0}
